= Fault Injection
include::_attributes.adoc[]

Apply some chaos engineering by throwing in some HTTP errors or network delays. Understanding failure scenarios is a critical aspect of microservices architecture (aka distributed computing)

[IMPORTANT]
.Before Start
====
You should have NO virtualservice other than the customer virtualservice. You also should have NO destinationrule (in `tutorial` namespace) `oc get virtualservice` `oc get destinationrule` 
if so run:

[source, bash]
----
oc delete -f istiofiles/lab6/virtual-service-recommendation-v1_and_v2.yml

oc delete -f istiofiles/lab6/destination-rule-recommendation-v1-v2.yml
----

====

[#503error]
== HTTP Error 503

By default, recommendation v1 and v2 are being randomly load-balanced as that is the default behavior in Kubernetes/OpenShift

[source,bash]
----
$ oc get pods -l app=recommendation 
or
$ kubectl get pods -l app=recommendation

NAME                                  READY     STATUS    RESTARTS   AGE
recommendation-v1-3719512284-7mlzw   2/2       Running   6          18h
recommendation-v2-2815683430-vn77w   2/2       Running   0          3h
----

You can inject 503's, for approximately 5% of the requests

[source,bash,subs="+macros,+attributes"]
----
oc create -f istiofiles/lab6/destination-rule-recommendation.yml

oc create -f istiofiles/lab6/virtual-service-recommendation-503.yml

curl ${CUSTOMER_URL}
customer => preference => recommendation v1 from '99634814-sf4cl': 88
curl ${CUSTOMER_URL}
customer => 503 preference => 503 fault filter abort
curl ${CUSTOMER_URL}
/fcustomer => preference => recommendation v2 from '2819441432-qsp25': 51
----

Clean up

[source,bash]
----
oc delete -f istiofiles/lab6/virtual-service-recommendation-503.yml 
----

[#delay]
== Delay

The most insidious of possible distributed computing faults is not a "down" service but a service that is responding slowly, potentially causing a cascading failure in your network of services.

[source,bash,subs="+macros,+attributes"]
----
oc create -f istiofiles/lab6/virtual-service-recommendation-delay.yml
----

And hit the customer endpoint

[source,bash]
----
./scripts/run.sh
----

You will notice many requests to the customer endpoint now have a delay.
If you are monitoring the logs for recommendation v1 and v2, you will also see the delay happens BEFORE the recommendation service is actually called

[source,bash]
----
oc logs recommendation-v1-c8796d6fb-ldm7s -c recommendation -f
----


Clean up

[source,bash]
----
oc delete -f istiofiles/lab6/destination-rule-recommendation.yml

oc delete -f istiofiles/lab6/virtual-service-recommendation-delay.yml
----

[#retry]
== Retry

Instead of failing immediately, retry the Service N more times

We will make pod recommendation-v2 fail 100% of the time:

[source,bash]
----
oc exec -it $(oc get pods|grep recommendation-v2|awk '{ print $1 }'|head -1) -c recommendation /bin/bash
or
kubectl exec -it $(oc get pods|grep recommendation-v2|awk '{ print $1 }'|head -1) -c recommendation /bin/bash
----

You will be inside the application container of your pod `recommendation-v2-2036617847-spdrb`. Now execute:

[source,bash]
----
curl 127.0.0.1:8080/misbehave
exit
----

This is a special endpoint that will make our application return only `503`s.

Now, if you hit the customer endpoint several times, you should see some 503's

[source,bash]
----
./scripts/run.sh

customer => preference => recommendation v1 from 'b87789c58-h9r4s': 864
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 865
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 866
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 867
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 868
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 869
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
----

Now add the retry rule

[source,bash,subs="+macros,+attributes"]
----
oc create -f istiofiles/lab6/virtual-service-recommendation-v2_retry.yml
----

You will see it work every time because Istio will retry the recommendation service and it will land on v1 only.

[source,bash]
----
./scripts/run.sh

customer => preference => recommendation v1 from '2036617847-m9glz': 196
customer => preference => recommendation v1 from '2036617847-m9glz': 197
customer => preference => recommendation v1 from '2036617847-m9glz': 198
----

You can see the active Virtual Services via

[source,bash]
----
oc get virtualservices 
----

Now, delete the retry rule and see the old behavior, where v2 throws 503s

[source,bash]
----
oc delete virtualservice recommendation 

./scripts/run.sh

customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1118
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1119
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1120
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1121
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1122
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1123
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1124
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1125
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
customer => preference => recommendation v1 from 'b87789c58-h9r4s': 1126
customer => 503 preference => 503 recommendation misbehavior from '6f64f9c5b-ltrhl'
----

Now, make the pod v2 behave well again

[source,bash]
----
oc exec -it $(oc get pods|grep recommendation-v2|awk '{ print $1 }'|head -1) -c recommendation /bin/bash
or
kubectl exec -it $(oc get pods|grep recommendation-v2|awk '{ print $1 }'|head -1) -c recommendation /bin/bash
----

You will be inside the application container of your pod `recommendation-v2-2036617847-spdrb`. Now execute:

[source,bash]
----
curl 127.0.0.1:8080/behave
exit
----

The application is back to random load-balancing between v1 and v2

[source,bash]
----
./scripts/run.sh

customer => preference => recommendation v1 from '2039379827-h58vw': 129
customer => preference => recommendation v2 from '2036617847-m9glz': 207
customer => preference => recommendation v1 from '2039379827-h58vw': 130
----

Clean up if needed.
